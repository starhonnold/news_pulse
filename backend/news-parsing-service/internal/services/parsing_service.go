package services

import (
	"context"
	"fmt"
	"sync"
	"time"

	"github.com/robfig/cron/v3"
	"github.com/sirupsen/logrus"

	"news-parsing-service/internal/config"
	"news-parsing-service/internal/models"
	"news-parsing-service/internal/repository"
)

// ParsingService –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ä–≤–∏—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
type ParsingService struct {
	rssParser            *RSSParser
	newsSourceRepo       *repository.NewsSourceRepository
	newsRepo             *repository.NewsRepository
	parsingLogRepo       *repository.ParsingLogRepository
	config               *config.ParsingConfig
	logger               *logrus.Logger
	cron                 *cron.Cron
	isRunning            bool
	mu                   sync.RWMutex
	semaphore            chan struct{}
	countryDetector      *CountryDetector
	contentExtractor     *ContentExtractor
	simpleNewsClassifier *WeightedNewsClassifier
	fastTextClient       *FastTextClassifierClient // –û—Å–Ω–æ–≤–Ω–æ–π FastText –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
}

// NewParsingService —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å –ø–∞—Ä—Å–∏–Ω–≥–∞
func NewParsingService(
	rssParser *RSSParser,
	newsSourceRepo *repository.NewsSourceRepository,
	newsRepo *repository.NewsRepository,
	parsingLogRepo *repository.ParsingLogRepository,
	parsingConfig *config.ParsingConfig,
	fullConfig *config.Config,
	logger *logrus.Logger,
) *ParsingService {
	// –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ä—Å–∏–Ω–≥–æ–≤
	semaphore := make(chan struct{}, parsingConfig.MaxConcurrentParsers)

	// –°–æ–∑–¥–∞–µ–º cron –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–¥–∞—á
	cronScheduler := cron.New(cron.WithSeconds())

	// –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–µ–∫—Ç–æ—Ä —Å—Ç—Ä–∞–Ω
	countryDetector := NewCountryDetector(logger)

	// –°–æ–∑–¥–∞–µ–º –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞
	contentExtractor, err := NewContentExtractor(logger, nil)
	if err != nil {
		logger.WithError(err).Error("Failed to create content extractor, but continuing")
		contentExtractor = nil
	}

	// –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å —â–∞–¥—è—â–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
	simpleNewsClassifier, err := NewWeightedNewsClassifier(logger, WeightedClassifierConfig{
		TitleWeight:   1.6,
		SummaryWeight: 1.0,
		ContentWeight: 1.0,
		MinConfidence: 0.18, // –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
		MinMargin:     0.03, // –°–Ω–∏–∂–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ—Ç—Ä—ã–≤
		UseStemming:   true,
		URLPriorBoost: 0.30, // –ü—Ä–∏–æ—Ä –ø–æ URL
		BatchTimeout:  30 * time.Second,

		// –©–∞–¥—è—â–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
		AllowUnknown:        true,
		MinScoreForFallback: 0.30,
		FallbackCategory:    CatSociety,
	})
	if err != nil {
		logger.WithError(err).Fatal("Failed to create news classifier")
	}

	// –°–æ–∑–¥–∞–µ–º FastText –∫–ª–∏–µ–Ω—Ç (NEW - –æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä)
	var fastTextClient *FastTextClassifierClient
	if fullConfig != nil && fullConfig.FastText.Enabled {
		serviceURL := fullConfig.FastText.ServiceURL
		timeout := fullConfig.FastText.Timeout
		if timeout == 0 {
			timeout = 30 * time.Second
		}

		fastTextClient = NewFastTextClassifierClient(serviceURL, timeout, logger, true)

		// –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–∞
		ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
		defer cancel()

		if err := fastTextClient.HealthCheck(ctx); err != nil {
			logger.WithError(err).Warn("FastText service unavailable, will use fallback classifier")
			fastTextClient.SetEnabled(false)
		} else {
			logger.WithFields(logrus.Fields{
				"url":     serviceURL,
				"timeout": timeout,
			}).Info("‚úÖ FastText classifier initialized and available")
		}
	} else {
		logger.Info("FastText classifier is disabled in config")
	}

	return &ParsingService{
		rssParser:            rssParser,
		newsSourceRepo:       newsSourceRepo,
		newsRepo:             newsRepo,
		parsingLogRepo:       parsingLogRepo,
		config:               parsingConfig,
		logger:               logger,
		cron:                 cronScheduler,
		semaphore:            semaphore,
		countryDetector:      countryDetector,
		contentExtractor:     contentExtractor,
		simpleNewsClassifier: simpleNewsClassifier,
		fastTextClient:       fastTextClient,
	}
}

// Start –∑–∞–ø—É—Å–∫–∞–µ—Ç —Å–µ—Ä–≤–∏—Å –ø–∞—Ä—Å–∏–Ω–≥–∞
func (s *ParsingService) Start() error {
	s.mu.Lock()
	defer s.mu.Unlock()

	if s.isRunning {
		return fmt.Errorf("parsing service is already running")
	}

	// –ó–∞–ø—É—Å–∫–∞–µ–º cron –∑–∞–¥–∞—á—É –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
	_, err := s.cron.AddFunc("@every 10m", s.ParseAllSources)
	if err != nil {
		return fmt.Errorf("failed to add cron job: %w", err)
	}

	s.cron.Start()
	s.isRunning = true

	s.logger.Info("Parsing service started")

	// –ó–∞–ø—É—Å–∫–∞–µ–º –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
	go s.ParseAllSources()

	return nil
}

// Stop –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–µ—Ä–≤–∏—Å –ø–∞—Ä—Å–∏–Ω–≥–∞
func (s *ParsingService) Stop() error {
	s.mu.Lock()
	defer s.mu.Unlock()

	if !s.isRunning {
		return fmt.Errorf("parsing service is not running")
	}

	s.cron.Stop()
	s.isRunning = false

	s.logger.Info("Parsing service stopped")
	return nil
}

// IsRunning –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Ä–∞–±–æ—Ç—ã —Å–µ—Ä–≤–∏—Å–∞
func (s *ParsingService) IsRunning() bool {
	return s.isRunning
}

// GetStats –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞—Ä—Å–∏–Ω–≥–∞
func (s *ParsingService) GetStats() map[string]interface{} {
	return map[string]interface{}{
		"is_running": s.isRunning,
	}
}

// ValidateSource –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
func (s *ParsingService) ValidateSource(source models.NewsSource) error {
	// –ü—Ä–æ—Å—Ç–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
	if source.RSSURL == "" {
		return fmt.Errorf("RSS URL is required")
	}
	return nil
}

// GetFeedInfo –ø–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∏–¥–µ
func (s *ParsingService) GetFeedInfo(ctx context.Context, url string) (map[string]interface{}, error) {
	// –ó–∞–≥–ª—É—à–∫–∞
	return map[string]interface{}{
		"url":         url,
		"title":       "Feed Title",
		"description": "Feed Description",
	}, nil
}

// ExtractContent –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ —Å—Ç–∞—Ç—å–∏
func (s *ParsingService) ExtractContent(ctx context.Context, url string) (string, error) {
	// –ó–∞–≥–ª—É—à–∫–∞
	return "Extracted content", nil
}

// ParseAllSources –ø–∞—Ä—Å–∏—Ç –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
func (s *ParsingService) ParseAllSources() {
	s.logger.Info("Starting to parse all sources")

	sources, err := s.newsSourceRepo.GetActive(context.Background())
	if err != nil {
		s.logger.WithError(err).Error("Failed to get active sources")
		return
	}

	if len(sources) == 0 {
		s.logger.Warn("No active sources found")
		return
	}

	s.logger.WithField("sources_count", len(sources)).Info("Found sources to parse")

	// –ü–∞—Ä—Å–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
	var wg sync.WaitGroup
	for _, source := range sources {
		wg.Add(1)
		go func(src models.NewsSource) {
			defer wg.Done()
			s.ParseSource(context.Background(), src)
		}(source)
	}

	wg.Wait()
	s.logger.Info("Finished parsing all sources")
}

// ParseSource –ø–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
func (s *ParsingService) ParseSource(ctx context.Context, source models.NewsSource) {
	// –ü–æ–ª—É—á–∞–µ–º —Å–µ–º–∞—Ñ–æ—Ä
	s.semaphore <- struct{}{}
	defer func() { <-s.semaphore }()

	startTime := time.Now()
	s.logger.WithField("source", source.Name).Info("Starting to parse source")

	// –ü–∞—Ä—Å–∏–º RSS –ª–µ–Ω—Ç—É
	result := s.rssParser.ParseFeed(ctx, source)
	if !result.Success {
		s.logger.WithFields(logrus.Fields{
			"source":  source.Name,
			"rss_url": source.RSSURL,
			"error":   result.Error,
		}).Error("Failed to parse RSS feed")
		return
	}
	items := result.Items

	if len(items) == 0 {
		s.logger.WithField("source", source.Name).Warn("No items found in RSS feed")
		return
	}

	// –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã
	newsList, err := s.processItems(ctx, items, source)
	if err != nil {
		s.logger.WithFields(logrus.Fields{
			"source": source.Name,
			"error":  err,
		}).Error("Failed to process items")
		return
	}

	// –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–º (—Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
	itemsFoundCount := len(items)

	if err := s.newsRepo.CreateBatch(ctx, newsList); err != nil {
		s.logger.WithFields(logrus.Fields{
			"source": source.Name,
			"error":  err,
		}).Error("Failed to create news batch")

		// –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –ø–∞—Ä—Å–∏–Ω–≥–∞
		parsingLog := models.ParsingLog{
			SourceID:        source.ID,
			Status:          "error",
			ErrorMessage:    err.Error(),
			ExecutionTimeMs: int(time.Since(startTime).Milliseconds()),
		}
		if err := s.parsingLogRepo.Create(ctx, &parsingLog); err != nil {
			s.logger.WithError(err).Warn("Failed to create parsing log")
		}
		return
	}

	// –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–µ–π –±—ã–ª–æ —Å–æ–∑–¥–∞–Ω–æ (–Ω–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç—ã)
	createdCount := 0
	for _, news := range newsList {
		if news.ID > 0 {
			createdCount++
		}
	}

	// –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞
	parsingLog := models.ParsingLog{
		SourceID:        source.ID,
		Status:          "success",
		NewsCount:       createdCount,
		ExecutionTimeMs: int(time.Since(startTime).Milliseconds()),
	}

	if err := s.parsingLogRepo.Create(ctx, &parsingLog); err != nil {
		s.logger.WithError(err).Warn("Failed to create parsing log")
	}

	s.logger.WithFields(logrus.Fields{
		"source":        source.Name,
		"items_found":   itemsFoundCount,
		"items_created": createdCount,
		"parse_time":    time.Since(startTime),
	}).Info("Successfully parsed source")
}

// processItems –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ RSS –ª–µ–Ω—Ç—ã –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∏—Ö –≤ –Ω–æ–≤–æ—Å—Ç–∏
func (s *ParsingService) processItems(ctx context.Context, items []models.ParsedFeedItem, source models.NewsSource) ([]models.News, error) {
	var newsList []models.News

	// –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –Ω–æ–≤–æ—Å—Ç—å –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏
	for _, item := range items {
		// –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è –Ω–æ–≤–æ—Å—Ç—å
		if s.config.EnableDeduplication {
			exists, err := s.newsRepo.ExistsByURL(ctx, item.Link, source.ID)
			if err != nil {
				s.logger.WithError(err).Warn("Failed to check news existence")
				continue
			}
			if exists {
				continue
			}
		}

		// –°–ù–ê–ß–ê–õ–ê –∏–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
		var fullContent string
		var contentSource string

		// –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
		if s.contentExtractor != nil && s.contentExtractor.IsValidURL(item.Link) {
			s.logger.WithField("url", item.Link).Debug("Attempting to extract content with go-readability")
			if extractedContent, err := s.contentExtractor.ExtractFullContent(ctx, item.Link); err == nil && extractedContent != "" {
				fullContent = extractedContent
				contentSource = "web_extraction"
				s.logger.WithFields(logrus.Fields{
					"url":            item.Link,
					"content_length": len(extractedContent),
					"source":         contentSource,
				}).Info("Successfully extracted full content from web page")
			} else {
				s.logger.WithFields(logrus.Fields{
					"url":   item.Link,
					"error": err,
				}).Warn("Failed to extract content from web page")
			}
		}

		// –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ, —á—Ç–æ –ø—Ä–∏—à–ª–æ –∏–∑ RSS
		if fullContent == "" {
			fullContent = item.Description
			contentSource = "rss_description"
		}

		// –°–¢–†–û–ì–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø: —Ç—Ä–µ–±—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ + –æ–ø–∏—Å–∞–Ω–∏–µ + –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
		// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∫—Ä–∏—Ç–µ—Ä–∏—è–º:
		// 1. –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ (—É–∂–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –≤ RSS –ø–∞—Ä—Å–µ—Ä–µ)
		// 2. –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ (–Ω–µ –ø—É—Å—Ç–æ–µ)
		// 3. –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–º–∏–Ω–∏–º—É–º 500 —Å–∏–º–≤–æ–ª–æ–≤)

		if item.Description == "" {
			s.logger.WithFields(logrus.Fields{
				"title": truncateForLog(item.Title, 50),
				"url":   item.Link,
			}).Debug("Skipping news without description")
			continue
		}

		if len(fullContent) < 500 {
			s.logger.WithFields(logrus.Fields{
				"title":          truncateForLog(item.Title, 50),
				"url":            item.Link,
				"content_length": len(fullContent),
			}).Debug("Skipping news with insufficient content (< 500 chars)")
			continue
		}

		// –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â—É—é –¥–ª–∏–Ω—É –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
		totalContentLength := len(item.Description) + len(fullContent)
		if totalContentLength < 700 {
			s.logger.WithFields(logrus.Fields{
				"title":                truncateForLog(item.Title, 50),
				"url":                  item.Link,
				"description_length":   len(item.Description),
				"content_length":       len(fullContent),
				"total_content_length": totalContentLength,
			}).Debug("Skipping news with insufficient total content (< 700 chars)")
			continue
		}

		// –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
		var categoryID *int

		// 1) FastText –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –ü–û–õ–ù–´–ú –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
		if s.fastTextClient != nil && s.fastTextClient.IsEnabled() {
			// –û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ + –æ–ø–∏—Å–∞–Ω–∏–µ + –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
			// –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—ë –¥–æ—Å—Ç—É–ø–Ω–æ–µ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
			textForClassification := item.Title + ". " + item.Description

			// –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–µ—Å–ª–∏ –æ–Ω –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–ø–∏—Å–∞–Ω–∏—è)
			if fullContent != "" && fullContent != item.Description {
				textForClassification += ". " + fullContent
			}

			// –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (FastText –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ –Ω–∞ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö)
			// –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤ - —ç—Ç–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
			if len(textForClassification) > 5000 {
				textForClassification = textForClassification[:5000]
			}

			// –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ confidence 65% - —Ç–∞–∫ –∫–∞–∫ —Ç–µ–ø–µ—Ä—å –≤—Å–µ–≥–¥–∞ –µ—Å—Ç—å –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
			minConfidence := 0.65

			s.logger.WithFields(logrus.Fields{
				"title":              truncateForLog(item.Title, 50),
				"description_length": len(item.Description),
				"content_length":     len(fullContent),
				"total_length":       len(textForClassification),
				"min_confidence":     minConfidence,
			}).Debug("Classifying with FastText (full content)")

			resp, err := s.fastTextClient.Classify(ctx, item.Title, textForClassification)

			if err == nil && resp.CategoryID > 0 && resp.Confidence >= minConfidence {
				categoryID = &resp.CategoryID
				s.logger.WithFields(logrus.Fields{
					"title":              truncateForLog(item.Title, 50),
					"category_id":        resp.CategoryID,
					"category_name":      resp.CategoryName,
					"confidence":         resp.Confidence,
					"original_category":  resp.OriginalCategory,
					"description_len":    len(item.Description),
					"content_len":        len(fullContent),
					"classification_len": len(textForClassification),
				}).Info("‚úÖ News classified with FastText (full content)")
			} else if err != nil {
				s.logger.WithFields(logrus.Fields{
					"title": truncateForLog(item.Title, 50),
					"error": err,
				}).Warn("FastText classification failed, using fallback")
			} else if resp.Confidence < minConfidence {
				s.logger.WithFields(logrus.Fields{
					"title":          truncateForLog(item.Title, 50),
					"confidence":     resp.Confidence,
					"min_confidence": minConfidence,
					"reason":         "confidence too low",
				}).Debug("FastText confidence too low, using fallback")
			}
		}

		// 2) Fallback –Ω–∞ "–û–±—â–µ—Å—Ç–≤–æ" –µ—Å–ª–∏ FastText –Ω–µ —É–≤–µ—Ä–µ–Ω
		// WeightedClassifier –û–¢–ö–õ–Æ–ß–ï–ù - –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö—É–∂–µ FastText
		if categoryID == nil {
			fallbackCategory := CatSociety // 5 ‚Äî –û–±—â–µ—Å—Ç–≤–æ
			categoryID = &fallbackCategory
			s.logger.WithFields(logrus.Fields{
				"title":       truncateForLog(item.Title, 50),
				"fallback_id": fallbackCategory,
				"reason":      "FastText confidence too low or error, using fallback",
			}).Info("üìã Using fallback category (–û–±—â–µ—Å—Ç–≤–æ)")
		}

		// –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω—É
		var detectedCountry *string
		if fullContent != "" {
			detectedCountry = s.countryDetector.DetectCountry(item.Title, item.Description, fullContent)
		}

		// –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –Ω–æ–≤–æ—Å—Ç–∏
		news := models.News{
			Title:          item.Title,
			Description:    s.ensureString(item.Description),
			Content:        s.ensureString(fullContent),
			URL:            item.Link,
			ImageURL:       s.ensureString(item.ImageURL),
			Author:         s.ensureString(item.Author),
			SourceID:       source.ID,
			CategoryID:     categoryID,
			PublishedAt:    item.Published,
			RelevanceScore: s.calculateRelevanceScore(item),
			IsActive:       true,
		}

		// –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
		s.logger.WithFields(logrus.Fields{
			"title":            truncateForLog(item.Title, 50),
			"category_id":      categoryID,
			"detected_country": detectedCountry,
			"content_length":   len(fullContent),
			"content_source":   contentSource,
			"source_id":        source.ID,
		}).Debug("Processed news item")

		// –í–∞–ª–∏–¥–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç—å
		if !news.IsValid() {
			s.logger.WithFields(logrus.Fields{
				"title":     item.Title,
				"url":       item.Link,
				"source_id": source.ID,
			}).Debug("Skipping invalid news")
			continue
		}

		// –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫
		newsList = append(newsList, news)
	}

	return newsList, nil
}

// calculateRelevanceScore –≤—ã—á–∏—Å–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏
func (s *ParsingService) calculateRelevanceScore(item models.ParsedFeedItem) float64 {
	score := 0.5 // –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª

	// –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –±–∞–ª–ª –∑–∞ –Ω–∞–ª–∏—á–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
	if item.ImageURL != "" {
		score += 0.1
	}

	// –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –±–∞–ª–ª –∑–∞ –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
	if len(item.Content) > 100 {
		score += 0.1
	}
	if len(item.Content) > 500 {
		score += 0.1
	}

	// –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –±–∞–ª–ª –∑–∞ –Ω–∞–ª–∏—á–∏–µ –∞–≤—Ç–æ—Ä–∞
	if item.Author != "" {
		score += 0.1
	}

	// –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –±–∞–ª–ª –¥–æ 1.0
	if score > 1.0 {
		score = 1.0
	}

	return score
}

// extractFullContent –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
func (s *ParsingService) extractFullContent(ctx context.Context, url string) (string, error) {
	if s.contentExtractor == nil {
		return "", fmt.Errorf("content extractor not available")
	}

	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
	defer cancel()

	content, err := s.contentExtractor.ExtractFullContent(ctx, url)
	if err != nil {
		return "", fmt.Errorf("failed to extract content: %w", err)
	}

	return content, nil
}

// ensureString –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Å—Ç—Ä–æ–∫–∞ –Ω–µ –ø—É—Å—Ç–∞—è (–∑–∞–º–µ–Ω—è–µ—Ç –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –Ω–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É)
func (s *ParsingService) ensureString(str string) string {
	if str == "" {
		return ""
	}
	return str
}

// truncateForLog –æ–±—Ä–µ–∑–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
func truncateForLog(s string, maxLen int) string {
	if len(s) <= maxLen {
		return s
	}
	return s[:maxLen] + "..."
}
